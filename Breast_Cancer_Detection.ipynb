{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download -d aryashah2k/breast-ultrasound-images-dataset\n",
        "!unzip breast-ultrasound-images-dataset.zip"
      ],
      "metadata": {
        "id": "o1EiO0yPZ41E"
      },
      "id": "o1EiO0yPZ41E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1467c58",
      "metadata": {
        "id": "f1467c58"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import glob as gb\n",
        "import keras\n",
        "import keras.layers as layers\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout, Activation, MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97a3d4de",
      "metadata": {
        "id": "97a3d4de"
      },
      "outputs": [],
      "source": [
        "path = \"Dataset_BUSI_with_GT/\"\n",
        "\n",
        "def encode(f):\n",
        "    labels = {'malignant': 0, 'benign': 1, 'normal': 2}\n",
        "    return labels[f]\n",
        "\n",
        "imageSize = 180\n",
        "\n",
        "\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for folder in os.listdir(path):\n",
        "  for file in gb.glob(path + folder + \"/*.png\"):\n",
        "      if \"mask\" not in file:\n",
        "        image = cv2.imread(file)\n",
        "        image = cv2.resize(image, (imageSize, imageSize))\n",
        "        image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # feature extraction\n",
        "        image_blur = cv2.GaussianBlur(image_gray, (7, 7), 0)\n",
        "        _, thresh_image = cv2.threshold(image_blur, 100, 255, cv2.THRESH_BINARY)\n",
        "        contours, hierarchy = cv2.findContours(thresh_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        image_contour = cv2.drawContours(image_gray, contours, -1, (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "        X.append(image_contour)\n",
        "        Y.append(encode(folder))\n",
        "\n",
        "\n",
        "\n",
        "X = np.array(X)\n",
        "\n",
        "# Normalization\n",
        "X = X/255.0\n",
        "\n",
        "Y = np.array(Y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, shuffle= False)"
      ],
      "metadata": {
        "id": "aZCCkR10dn97"
      },
      "id": "aZCCkR10dn97",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CNN Models**"
      ],
      "metadata": {
        "id": "AswDh--VbyIu"
      },
      "id": "AswDh--VbyIu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "884d4899",
      "metadata": {
        "id": "884d4899"
      },
      "outputs": [],
      "source": [
        "\"\"\"Custom CNN Archeticure\"\"\"\n",
        "\n",
        "CNNmodel = Sequential(\n",
        "    [\n",
        "        Conv2D(64, (3, 3), input_shape=(imageSize, imageSize, 1), padding='same', activation='relu'),\n",
        "        MaxPool2D(pool_size=(2, 2)),\n",
        "        Conv2D(64, (3, 3), padding = 'same', activation='relu'),\n",
        "        MaxPool2D(pool_size=(2, 2)),\n",
        "        Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
        "        MaxPool2D(pool_size=(2, 2)),\n",
        "        Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
        "        MaxPool2D(pool_size=(2, 2)),\n",
        "        Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
        "        MaxPool2D(pool_size=(2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(units=512, activation='relu'),\n",
        "        Dense(units=256, activation='relu'),\n",
        "        Dense(units=128, activation='relu'),\n",
        "        Dense(units=3, activation='softmax'),\n",
        "    ]\n",
        "\n",
        ")\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "k = 0\n",
        "scores = []\n",
        "inputs = np.concatenate((x_train, x_test), axis=0)\n",
        "targets = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "\n",
        "\n",
        "for train, test in kf.split(inputs, targets):\n",
        "    k= k + 1\n",
        "    CNNmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    history = CNNmodel.fit(inputs[train], targets[train], epochs=10, validation_data=(inputs[test], targets[test]))\n",
        "    _, score = CNNmodel.evaluate(inputs[test], targets[test])\n",
        "    model_json = CNNmodel.to_json()\n",
        "    with open(str(k)+\".json\", \"w\") as json_file:\n",
        "      json_file.write(model_json)\n",
        "      CNNmodel.save_weights(str(k)+\".h5\")\n",
        "      json_file.close()\n",
        "\n",
        "    # summarize history for accuracy\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.savefig(str(k)+\" Accuracy\"+'.png', dpi=1000)\n",
        "    plt.show()\n",
        "\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.savefig(str(k)+\" Loss\"+'.png', dpi=1000)\n",
        "    plt.show()\n",
        "\n",
        "    scores.append(score)\n",
        "\n",
        "\n",
        "print(np.mean(scores))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ec645e2",
      "metadata": {
        "id": "1ec645e2"
      },
      "outputs": [],
      "source": [
        "\"\"\"letnet-5 Archeticure\"\"\"\n",
        "\n",
        "letnet_5 = keras.Sequential()\n",
        "letnet_5.add(layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(180,180,1)))\n",
        "letnet_5.add(layers.AveragePooling2D())\n",
        "letnet_5.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n",
        "letnet_5.add(layers.AveragePooling2D())\n",
        "letnet_5.add(layers.Flatten())\n",
        "letnet_5.add(layers.Dense(units=120, activation='relu'))\n",
        "letnet_5.add(layers.Dense(units=84, activation='relu'))\n",
        "letnet_5.add(layers.Dense(units=3, activation = 'softmax'))\n",
        "letnet_5.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "letnet_5.summary()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "k = 0\n",
        "scores = []\n",
        "inputs = np.concatenate((x_train, x_test), axis=0)\n",
        "targets = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "\n",
        "for train, test in kf.split(inputs, targets):\n",
        "    k= k + 1\n",
        "    letnet_5.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    history = letnet_5.fit(inputs[train], targets[train], epochs=10, validation_data=(inputs[test], targets[test]))\n",
        "    _, score = letnet_5.evaluate(inputs[test], targets[test])\n",
        "    model_json = letnet_5.to_json()\n",
        "\n",
        "    with open(str(k)+\".json\", \"w\") as json_file:\n",
        "      json_file.write(model_json)\n",
        "      letnet_5.save_weights(str(k)+\".h5\")\n",
        "      json_file.close()\n",
        "\n",
        "    # summarize history for accuracy\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.savefig(str(k)+\" Accuracy\"+'.png', dpi=1000)\n",
        "    plt.show()\n",
        "\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.savefig(str(k)+\" Loss\"+'.png', dpi=1000)\n",
        "    plt.show()\n",
        "    scores.append(score)\n",
        "\n",
        "print(np.mean(scores))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab94d316",
      "metadata": {
        "id": "ab94d316"
      },
      "outputs": [],
      "source": [
        "\"\"\"AlexNet Archeticure\"\"\"\n",
        "\n",
        "\n",
        "#1st Convolutional Layer\n",
        "AlexNet = Sequential()\n",
        "AlexNet.add(Conv2D(filters=96, input_shape=(180,180,1), kernel_size=(11,11), strides=(4,4), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "#2nd Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "#3rd Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "\n",
        "#4th Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "\n",
        "#5th Convolutional Layer\n",
        "AlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
        "\n",
        "#Passing it to a Fully Connected layer\n",
        "AlexNet.add(Flatten())\n",
        "\n",
        "# 1st Fully Connected Layer\n",
        "AlexNet.add(Dense(4096, input_shape=(32,32,3,)))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "\n",
        "# Add Dropout to prevent overfitting\n",
        "AlexNet.add(Dropout(0.4))\n",
        "\n",
        "\n",
        "#2nd Fully Connected Layer\n",
        "AlexNet.add(Dense(4096))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "\n",
        "#Add Dropout\n",
        "AlexNet.add(Dropout(0.4))\n",
        "\n",
        "#3rd Fully Connected Layer\n",
        "AlexNet.add(Dense(1000))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('relu'))\n",
        "\n",
        "#Add Dropout\n",
        "AlexNet.add(Dropout(0.4))\n",
        "\n",
        "#Output Layer\n",
        "AlexNet.add(Dense(3))\n",
        "AlexNet.add(BatchNormalization())\n",
        "AlexNet.add(Activation('softmax'))\n",
        "\n",
        "#Model Summary\n",
        "AlexNet.summary()\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "k = 0\n",
        "scores = []\n",
        "inputs = np.concatenate((x_train, x_test), axis=0)\n",
        "targets = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "\n",
        "for train, test in kf.split(inputs, targets):\n",
        "    k= k + 1\n",
        "    AlexNet.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    history = AlexNet.fit(inputs[train], targets[train], epochs=10, validation_data=(inputs[test], targets[test]))\n",
        "    _, score = AlexNet.evaluate(inputs[test], targets[test])\n",
        "    model_json = AlexNet.to_json()\n",
        "    with open(str(k)+\".json\", \"w\") as json_file:\n",
        "      json_file.write(model_json)\n",
        "      AlexNet.save_weights(str(k)+\".h5\")\n",
        "      json_file.close()\n",
        "\n",
        "\n",
        "\n",
        "    # summarize history for accuracy\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.savefig(str(k)+\" Accuracy\"+'.png', dpi=1000)\n",
        "    plt.show()\n",
        "\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.savefig(str(k)+\" Loss\"+'.png', dpi=1000)\n",
        "    plt.show()\n",
        "\n",
        "    scores.append(score)\n",
        "\n",
        "print(np.mean(scores))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Machine Learning Classifiers**"
      ],
      "metadata": {
        "id": "YPhuRFqJb94j"
      },
      "id": "YPhuRFqJb94j"
    },
    {
      "cell_type": "code",
      "source": [
        "x_train1 = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test1 = x_test.reshape(x_test.shape[0], -1)"
      ],
      "metadata": {
        "id": "0QXI5arAcDJw"
      },
      "id": "0QXI5arAcDJw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Support Vector Machine (SVM)\"\"\"\n",
        "\n",
        "clf = svm.SVC(kernel='linear', gamma='auto')\n",
        "clf.fit(x_train1, y_train)\n",
        "\n",
        "y_pred = clf.predict(x_test1)\n",
        "print(\"SVM Accuracy =\", accuracy_score(y_test, y_pred))\n",
        "print(\"SVM Precision Score = \", precision_score(y_test, y_pred, average='micro'))\n",
        "print(\"SVM Recall Score = \", recall_score(y_test, y_pred, average='micro'))\n",
        "\n",
        "svm_matrix = confusion_matrix(y_test, y_pred)\n",
        "ax = sns.heatmap(svm_matrix, annot=True, cmap='Blues')\n",
        "ax.set_xlabel('\\nPredicted Values')\n",
        "ax.set_ylabel('Actual Values ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels(['Benign','Malignant','Normal'])\n",
        "ax.yaxis.set_ticklabels(['Benign','Malignant','Normal'])\n",
        "\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.show()\n",
        "plt.savefig('SVM.png', dpi=1000)"
      ],
      "metadata": {
        "id": "TTqiqyyecTKn"
      },
      "id": "TTqiqyyecTKn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Naive Bayes\"\"\"\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(x_train1, y_train)\n",
        "gnb_predictions = gnb.predict(x_test1)\n",
        "\n",
        "print(\"Naive Bayes Accuracy =\", accuracy_score(y_test, gnb_predictions))\n",
        "print(\"Precision Score = \", precision_score(y_test, gnb_predictions, average='micro'))\n",
        "print(\"Recall Score = \", recall_score(y_test, gnb_predictions, average='micro'))\n",
        "\n",
        "\n",
        "gnb_matrix = confusion_matrix(y_test, gnb_predictions)\n",
        "ax = sns.heatmap(gnb_matrix, annot=True, cmap='Blues')\n",
        "ax.set_xlabel('\\nPredicted Values')\n",
        "ax.set_ylabel('Actual Values ');\n",
        "\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels(['Benign','Malignant','Normal'])\n",
        "ax.yaxis.set_ticklabels(['Benign','Malignant','Normal'])\n",
        "\n",
        "\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.show()\n",
        "plt.savefig('GNB.png', dpi=1000)"
      ],
      "metadata": {
        "id": "lmv29SB2c3XH"
      },
      "id": "lmv29SB2c3XH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7cd5278",
      "metadata": {
        "id": "c7cd5278"
      },
      "outputs": [],
      "source": [
        "\"\"\"k-Nearest Neighbors\"\"\"\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=9)\n",
        "knn.fit(x_train1, y_train)\n",
        "knn_predictions = knn.predict(x_test1)\n",
        "\n",
        "print(\"KNN Accuracy =\", accuracy_score(y_test, knn_predictions))\n",
        "print(\"KNN Precision Score = \", precision_score(y_test, knn_predictions, average='micro'))\n",
        "print(\"KNN Recall Score = \", recall_score(y_test, knn_predictions, average='micro'))\n",
        "\n",
        "\n",
        "knn_matrix = confusion_matrix(y_test, knn_predictions)\n",
        "ax = sns.heatmap(knn_matrix, annot=True, cmap='Blues')\n",
        "ax.set_xlabel('\\nPredicted Values')\n",
        "ax.set_ylabel('Actual Values ');\n",
        "\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels(['Benign','Malignant','Normal'])\n",
        "ax.yaxis.set_ticklabels(['Benign','Malignant','Normal'])\n",
        "\n",
        "\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.show()\n",
        "plt.savefig('KNN.png', dpi=1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Decision Tree\"\"\"\n",
        "\n",
        "DT = DecisionTreeClassifier()\n",
        "DT = DT.fit(x_train1, y_train)\n",
        "y_predicted = DT.predict(x_test1)\n",
        "\n",
        "print(\"Decision tree Accuracy =\", accuracy_score(y_test, y_predicted))\n",
        "print(\"Decision tree Precision Score = \", precision_score(y_test, y_predicted, average='micro'))\n",
        "print(\"Decision tree Score = \", recall_score(y_test, y_predicted, average='micro'))\n",
        "\n",
        "DT_matrix = confusion_matrix(y_test, y_predicted)\n",
        "ax = sns.heatmap(DT_matrix, annot=True, cmap='Blues')\n",
        "ax.set_xlabel('\\nPredicted Values')\n",
        "ax.set_ylabel('Actual Values ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "ax.xaxis.set_ticklabels(['Benign','Malignant','Normal'])\n",
        "ax.yaxis.set_ticklabels(['Benign','Malignant','Normal'])\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.show()\n",
        "plt.savefig('DT.png', dpi=1000)"
      ],
      "metadata": {
        "id": "_9yqT6DfdJRm"
      },
      "id": "_9yqT6DfdJRm",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}